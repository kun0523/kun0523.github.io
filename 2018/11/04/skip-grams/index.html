<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.5.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="算法思想： 一个词的意思应该是由上下文决定的； 所以努力根据一个词去生成一个向量，通过这个向量可以预测该词附近的词； 一个词有两个词向量，一个是作为中央词的词向量，一个是作为上下文的词向量；   对词汇表中的词进行one-hot编码; 通过词向量矩阵，将one-hot编码向量转化为density vector； 通过上下文向量矩阵，将词向量转化为一个与输入同型的向量，再经过$softmax$函数转">
<meta property="og:type" content="article">
<meta property="og:title" content="Skip-Grams">
<meta property="og:url" content="https:///kun0523.github.io/2018/11/04/skip-grams/index.html">
<meta property="og:site_name" content="Learning Notes">
<meta property="og:description" content="算法思想： 一个词的意思应该是由上下文决定的； 所以努力根据一个词去生成一个向量，通过这个向量可以预测该词附近的词； 一个词有两个词向量，一个是作为中央词的词向量，一个是作为上下文的词向量；   对词汇表中的词进行one-hot编码; 通过词向量矩阵，将one-hot编码向量转化为density vector； 通过上下文向量矩阵，将词向量转化为一个与输入同型的向量，再经过$softmax$函数转">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://wx1.sinaimg.cn/large/006Fmjmcly1fgco3v2ca7j30pq0j7drt.jpg">
<meta property="og:updated_time" content="2018-11-04T13:27:35.515Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Skip-Grams">
<meta name="twitter:description" content="算法思想： 一个词的意思应该是由上下文决定的； 所以努力根据一个词去生成一个向量，通过这个向量可以预测该词附近的词； 一个词有两个词向量，一个是作为中央词的词向量，一个是作为上下文的词向量；   对词汇表中的词进行one-hot编码; 通过词向量矩阵，将one-hot编码向量转化为density vector； 通过上下文向量矩阵，将词向量转化为一个与输入同型的向量，再经过$softmax$函数转">
<meta name="twitter:image" content="http://wx1.sinaimg.cn/large/006Fmjmcly1fgco3v2ca7j30pq0j7drt.jpg">






  <link rel="canonical" href="https:///kun0523.github.io/2018/11/04/skip-grams/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Skip-Grams | Learning Notes</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Learning Notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">日常笔记</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https:///kun0523.github.io/2018/11/04/skip-grams/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kun0523">
      <meta itemprop="description" content="include machine learning, math, psychology,  etc.">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Learning Notes">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Skip-Grams
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-04 21:17:12 / 修改时间：21:27:35" itemprop="dateCreated datePublished" datetime="2018-11-04T21:17:12+08:00">2018-11-04</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="算法思想："><a href="#算法思想：" class="headerlink" title="算法思想："></a>算法思想：</h3><ul>
<li>一个词的意思应该是由上下文决定的；</li>
<li>所以努力根据一个词去生成一个向量，通过这个向量可以预测该词附近的词；</li>
<li>一个词有两个词向量，一个是作为中央词的词向量，一个是作为上下文的词向量；</li>
</ul>
<ol>
<li>对词汇表中的词进行one-hot编码;</li>
<li>通过词向量矩阵，将one-hot编码向量转化为density vector；</li>
<li>通过上下文向量矩阵，将词向量转化为一个与输入同型的向量，再经过$softmax$函数转变为$0~1$之间的小数；</li>
<li>与几个已知的上下文词汇，计算相似度（向量内积），作为损失函数；</li>
<li>我们的目标就是获取其中的两个矩阵，词向量矩阵（相当于一个表，可以从中查到每一个词对应的词向量），上下文向量矩阵（也相当于一个表，查询该词对应的上下文词向量）；</li>
</ol>
<h3 id="算法推导："><a href="#算法推导：" class="headerlink" title="算法推导："></a>算法推导：</h3><h4 id="目标函数："><a href="#目标函数：" class="headerlink" title="目标函数："></a>目标函数：</h4><script type="math/tex; mode=display">J^\prime(\theta) = \prod_{t=1}^T\prod_{-m \leq j \geq m  j\neq 0} P(w_{t+j}|w_t; \theta)</script><blockquote>
<p><strong>公式解释：</strong>有T个字，分别看每个词前后2m个字（m称为窗口大小），计算在给定中心字$w<em>t$与参数$\theta$的情况下，预测到$w</em>{t+j}$的概率，目标是：通过迭代参数$\theta$的值，使得每一个中心字的上下文预测准确度最大化；</p>
</blockquote>
<script type="math/tex; mode=display">J(\theta) = -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \geq m ~ j \neq 0} log P(w_{t+j}|w_t)</script><blockquote>
<p><strong>公式解释：</strong>类似上一个公式，只是取了一个负对数，将原先的连乘转变为累加，并且从原先的求最大值，变为求最小值；</p>
</blockquote>
<h4 id="上下文词概率的计算："><a href="#上下文词概率的计算：" class="headerlink" title="上下文词概率的计算："></a>上下文词概率的计算：</h4><p><img src="http://wx1.sinaimg.cn/large/006Fmjmcly1fgco3v2ca7j30pq0j7drt.jpg" alt="skip-grams"></p>
<blockquote>
<p>图片解释：</p>
<ul>
<li>输入一个以one-hot编码的中央词汇，与词向量矩阵$W$点乘后得到词向量$v_c$  [$v_c = W \cdot w_t$]（由于输入时one-hot编码，实际是从一个矩阵中抽取一列）；</li>
<li>将中央词向量$v_c$与上下文词向量矩阵$W^\prime$点乘后得到词汇表中每一个词与中央词之间相似度大小向量$W^\prime v_c$;</li>
<li>将每一个词与中央词的相似度大小通过$softmax$函数转换为概率值，$p(x|c) = softmax(u_x^T v_c)$;</li>
<li>用每个词的概率向量与实际上下文词的one-hot向量比对，得到损失函数，通过优化减低损失值；</li>
<li>所以，模型中需要迭代优化的参数$\theta$是$W ~ and ~ W^\prime $； $\theta = [v<em>1, v_2, \ldots, v</em>{|V|}, u<em>1, u_2, \ldots, u</em>{|V|}]$</li>
<li>参数$d$是一个超参数，你想把词向量投影到多少维的空 间里；参数$|V|$是指词汇表中一共有多少个词；</li>
</ul>
<p>上下文词汇出现的概率：$P(w<em>{t+j}|w_t) = P(o|c)$ o代表<strong>observation</strong>指真实的上下文词汇<br>$P(o|c) = \frac{exp(u_o^T v_c)}{\sum</em>{w=1}^Vexp(u_w^Tv_c)}$该概率值处于$0\sim 1$之间，当等于1时，意味着$100\%$预测正确，所以努力让这个概率值最大化;</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial v_c} log P(o|c) = \frac{\partial}{\partial v_c}log \frac{exp(u_o^T v_c)}{\sum_{w=1}^V exp(u_w^T v_c)} = \frac{\partial}{\partial v_c}log >exp(u_o^Tv_c) - \frac{\partial}{\partial v_c}log \sum_{w=1}^V exp(u_w^T v_c)</script><script type="math/tex; mode=display">= \frac{\partial}{\partial v_c}u_o^Tv_c - \frac{\frac{\partial}{\partial v_c}\sum_{x=1}^Vexp(u_x^Tv_c)}{\sum_{w=1}^Vexp(u_w^T v_c)} = u_o - \sum_{x=1}^V >\frac{exp(u_x^T v_c)}{\sum_{w=1}^V exp(u_w^T v_c)}u_x = u_o - \sum_{x=1}^VP(x|c)u_x</script><ul>
<li>$u_o$: observation 即真实的上下文词汇的one-hot编码；</li>
<li>$\sum_{x=1}^VP(x|c)u_x$: expectation 表示一个期望值，上下文词x的概率乘该词的上下文词向量；</li>
<li>努力调整参数使得两者相等；</li>
</ul>
</blockquote>
<p>？？以上只是完成了对$v_c$的偏导，对上下文词汇矩阵参数要怎么求导？？</p>
<h4 id="隐藏层维度-d-怎么选取？"><a href="#隐藏层维度-d-怎么选取？" class="headerlink" title="隐藏层维度$d$怎么选取？"></a>隐藏层维度$d$怎么选取？</h4><h4 id="参数怎么初始化？"><a href="#参数怎么初始化？" class="headerlink" title="参数怎么初始化？"></a>参数怎么初始化？</h4><h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样:"></a>负采样:</h4><ul>
<li><p><strong>什么是负采样：</strong></p>
<p>每个正例搭配几个负例；（正例：中央词语与窗口内的词语对；负例：中央词语与窗口外的词语对；）</p>
</li>
<li><p><strong>为什么要负采样：</strong></p>
<p>因为词汇表（vocabulary）中的词可能非常多，导致上下文转化矩阵（$W^\prime \  \ V*d$)中的$V$很大，使得$softmax$函数计算困难（$softmax$函数分母上需要计算词库中所有词与中心词共现 的概率并求和），所以使用采样的方式；只从词库中随机抽取几个词，努力减小这些负样本与中心词共现的概率，减小计算量；</p>
</li>
<li><p><strong>怎么实现负采样：</strong></p>
<p>新的目标函数：</p>
<script type="math/tex; mode=display">J_t(\theta) = log \sigma(u_o^T v_c) + \sum_{j=1}^k \mathbb{E}_{j\sim P(w)}[log \sigma(-u_j^Tv_c)]</script><p>稍微简单一点的符号表示：</p>
<script type="math/tex; mode=display">J_t(\theta) = log\sigma(u_o^Tv_c) + \sum_{j\sim P(w)}[log(1-\sigma(u_j^Tv_c)]) = log\sigma(u_o^Tv_c) + \sum_{j\sim P(w)}[log\sigma(-u_j^Tv_c)]</script><p>疑惑？？</p>
<ul>
<li>是监督学习吗？？</li>
<li>监督发生在哪？？</li>
<li>$u_o, u_j$这些向量是参数矩阵里的一行，是怎么跟观测到的上下文词汇的one-hot编码联系起来的？？</li>
</ul>
<p>公式解释：</p>
<ul>
<li>$\theta$:指代模型中所有的参数集合， $t$:指代第t个时步，或第t次循环时的窗口；</li>
<li>$\sigma(x)$指代$sigmoid$函数，$\sigma(x) = {1 \over {1+e^{-x}}} $ ， 而且$sigmoid$函数有2 个特点是：$1-\sigma(x) = \sigma(-x)$；$\sigma(x)^\prime = \sigma(x)(1-\sigma(x)) = \sigma(x)\sigma(-x)$</li>
<li>$v_c$:是转换得到的中央词的词向量；</li>
<li>$u_o$:是上下文词汇向量；</li>
<li>$u_j$:负样本的上下文词向量，（按理说应该是严格不等于上下文词汇，但是相对于一个很大的词汇表，采样到上下文词汇的概率很低，所以通常的处理方式是<strong>随机抽取</strong>，不考虑排出上下文词汇的情况。）</li>
<li>$k$：是采样个数；</li>
<li>$P(w)$：采样服从的概率，通常$P(w) = U(w)^{3/4}/Z$，对均匀分布取$3 \over 4$次方是为了相对的提高采到低频词汇的概率，同时相对的降低采到高频<em>停用词</em>的概率；</li>
<li>目标是让<strong>第一项尽量大</strong>（观测到的<em>上下文词汇与中心词汇</em>共现的概率最大化），让<strong>第二项尽量大</strong>（随机抽到的<em>负样本与中心词汇</em>共现的概率尽量小）；</li>
</ul>
</li>
</ul>
<h3 id="GloVe："><a href="#GloVe：" class="headerlink" title="GloVe："></a>GloVe：</h3><ul>
<li>结合count base方法与direct prediction方法两者的优势，global vectors model：</li>
<li>count base: 统计语料库中的词汇之间的共现矩阵（很稀疏的矩阵），通过SVD降维获取词向量，从而计算不同词之间的相似度（相似度可以使用欧氏距离或者余弦值）<ul>
<li>可以很好的使用统计特性</li>
<li>对于小样本集训练快；</li>
<li>主要用于捕获词之间的相似性；</li>
<li></li>
</ul>
</li>
<li>direct prediction：基于窗口的skip-gram, CBOW算法，<ul>
<li>劣势：因为每个词都要计算它的窗口内的词，相当于放大了语料库；</li>
<li>劣势：不能很好的利用统计特性；</li>
<li>优势：在某些任务中表现更好，例如实体识别，词性标注；</li>
<li>优势：可以捕获到更复杂的模式特征；</li>
</ul>
</li>
</ul>
<h4 id="目标函数：-1"><a href="#目标函数：-1" class="headerlink" title="目标函数："></a>目标函数：</h4><script type="math/tex; mode=display">J(\theta) = \frac{1}{2}\sum_{i,j=1}^Wf(P_{ij})(u_i^Tv_j - logP_{ij})^2</script><h5 id="公式解释："><a href="#公式解释：" class="headerlink" title="公式解释："></a>公式解释：</h5><ul>
<li>$\theta$:指代模型中所有参数；</li>
<li>$u_i, v_j$:指代上下文词向量与中心词向量；</li>
<li>$i, j$:遍历词汇表共现矩阵中的每一个元素；</li>
<li>$P_{ij}$:指代在共现矩阵中，词$i, j$共现的<strong>频次</strong>；</li>
<li>$f(P_{ij})$:该函数是用来降低一些共现次数非常高的词对，$f(x) = minimum(x, target)$当出现频率x大于target值时，返回值不再增大；</li>
<li>目标是：努力使得$u<em>i^Tv_j$  和 $logP</em>{ij}$ 两个相近，使得损失函数最小（两个词<strong>共现频次大</strong>的，两个词向量的<strong>内积就应该相应的比较大</strong>；而两个词共现频次小的，两个词向量内积就应该相应的较小。）；</li>
</ul>
<h5 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h5><ul>
<li>训练快；</li>
<li>对于很小的语料库表现依然很好；</li>
</ul>
<h5 id="细节处理："><a href="#细节处理：" class="headerlink" title="细节处理："></a>细节处理：</h5><ul>
<li>$\vec u, \vec v$这个两个词向量在模型训练阶段分开训练比较好，但是在最终表征词时采用两者相加的方式效果最好，即$x_{final} = \vec u + \vec v$.</li>
</ul>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/04/latex-base/" rel="next" title="latex_base">
                <i class="fa fa-chevron-left"></i> latex_base
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Kun0523</p>
              <p class="site-description motion-element" itemprop="description">include machine learning, math, psychology,  etc.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#算法思想："><span class="nav-number">1.</span> <span class="nav-text">算法思想：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法推导："><span class="nav-number">2.</span> <span class="nav-text">算法推导：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数："><span class="nav-number">2.1.</span> <span class="nav-text">目标函数：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#上下文词概率的计算："><span class="nav-number">2.2.</span> <span class="nav-text">上下文词概率的计算：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层维度-d-怎么选取？"><span class="nav-number">2.3.</span> <span class="nav-text">隐藏层维度$d$怎么选取？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数怎么初始化？"><span class="nav-number">2.4.</span> <span class="nav-text">参数怎么初始化？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#负采样"><span class="nav-number">2.5.</span> <span class="nav-text">负采样:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GloVe："><span class="nav-number">3.</span> <span class="nav-text">GloVe：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数：-1"><span class="nav-number">3.1.</span> <span class="nav-text">目标函数：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#公式解释："><span class="nav-number">3.1.1.</span> <span class="nav-text">公式解释：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#优缺点："><span class="nav-number">3.1.2.</span> <span class="nav-text">优缺点：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#细节处理："><span class="nav-number">3.1.3.</span> <span class="nav-text">细节处理：</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright"> &copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kun0523</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.5.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.5.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.5.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script>



  



  










  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
